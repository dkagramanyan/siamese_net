{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout,Activation, Lambda,LeakyReLU,ReLU\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,BatchNormalization,LayerNormalization,GlobalMaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from image_utils import get_pairs,get_dataset_info,estimate_dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define images main information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_name='datasets/sof_preproc'\n",
    "get_dataset_info(main_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset parametrs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape=(128,128,3)\n",
    "max_positive_pairs_count=20000\n",
    "max_negative_pairs_count=20000\n",
    "estimate_dataset(main_folder_name,\n",
    "                 max_positive_pairs_count,\n",
    "                 max_negative_pairs_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocces the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs,labels=get_pairs(main_folder_name,\n",
    "                       max_positive_pairs_count,\n",
    "                       max_negative_pairs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs=pairs.astype(np.float32)\n",
    "labels=labels.astype(np.float32)\n",
    "\n",
    "train_pairs,test_pairs,train_labels,test_labels=train_test_split(pairs,labels,test_size=0.05)\n",
    "del pairs\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese pecularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As siamese network measure distance between we define Euclidean distance Lambda layer and special contrastive loss ( https://arxiv.org/abs/2011.02803 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "    \n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return K.mean( (1 - y_true) * square_pred+y_true*margin_square)\n",
    "    \n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    #\n",
    "    #   it is also good idea to use VGG model, but it requires powerfull GPU\n",
    "    #   model = Sequential(VGG16(weights='imagenet', include_top=False, input_shape=image_shape).layers)\n",
    "    #\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32,(3,3),input_shape=image_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32,(3,3)))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "        \n",
    "    model.add(Conv2D(64,(3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(3,3)))\n",
    "    model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "    model.add(Conv2D(128,(3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128,(3,3)))\n",
    "    model.add(MaxPooling2D((3,3)))\n",
    "    \n",
    "    model.add(Conv2D(256,(3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(256,(3,3)))\n",
    "    model.add(MaxPooling2D((3,3)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#base_network = get_model()\n",
    "resnet = tf.keras.applications.ResNet50V2(weights = 'imagenet', include_top = False, input_shape = (128,128,3))\n",
    "x = Flatten()(resnet.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = LeakyReLU(0.2)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "output = LeakyReLU(0.2)(x)\n",
    "\n",
    "\n",
    "base_model = Model(inputs = resnet.input, outputs = output)\n",
    "\n",
    "base_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define siamese model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide weights sycronization we define left and right inputs with the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_a = Input(shape=image_shape)\n",
    "vect_output_a = base_model(input_a)\n",
    "\n",
    "input_b = Input(shape=image_shape)\n",
    "vect_output_b = base_model(input_b)\n",
    "\n",
    "x = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([vect_output_a, vect_output_b])\n",
    "output= Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = Model([input_a, input_b], output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = RMSprop(  learning_rate=0.0001)\n",
    "#optim = Adam(  learning_rate=0.015)\n",
    "model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer=optim)\n",
    "history = model.fit([train_pairs[:,0],train_pairs[:,1]], \n",
    "                    train_labels, \n",
    "                    epochs=17, \n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path='saved data'\n",
    "\n",
    "weights_name='human_weight.h5'\n",
    "history_name='human_history.json'\n",
    "\n",
    "hist_json_file = saved_path+'/'+history_name\n",
    "weights_file=saved_path+'/' +weights_name\n",
    "\n",
    "base_model.save_weights(weights_file)\n",
    "hist_df = pd.DataFrame(history.history)  \n",
    "\n",
    "with open(hist_json_file, mode='w') as f:\n",
    "    hist_df.to_json(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path='saved data'\n",
    "\n",
    "weights_name='human_weight.h5'\n",
    "history_name='human_history.json'\n",
    "\n",
    "hist_json_file = saved_path+'/'+history_name\n",
    "weights_file=saved_path+'/' +weights_name\n",
    "\n",
    "base_model.load_weights(weights_file)\n",
    "loaded_history=pd.read_json(hist_json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loaded_history,title):\n",
    "    loss=loaded_history['loss']\n",
    "    val_loss=loaded_history['val_loss']\n",
    "    x=range(len(loss))\n",
    "    X=15\n",
    "    Y=8\n",
    "    plt.figure(figsize=(X,Y))\n",
    "    plt.plot(x,loss,'b',x,val_loss,'g')\n",
    "    plt.title(title, fontsize=20, fontname='Times New Roman')\n",
    "    plt.legend(['loss','val_loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title='sof losses per epoch'\n",
    "plot_loss(loaded_history,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "\n",
    "To compute accuracy of the model define rule: if predicted value is bigger, then 0.5, the result is 1, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() > 0.5\n",
    "    return round(np.mean(pred == y_true),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = round(model.evaluate(x=[test_pairs[:,0],test_pairs[:,1]], y=test_labels),4)\n",
    "\n",
    "y_pred_train = model.predict([test_pairs[:,0], test_pairs[:,1]])\n",
    "train_accuracy = compute_accuracy(test_labels, y_pred_train)\n",
    "\n",
    "y_pred_test = model.predict([test_pairs[:,0], test_pairs[:,1]])\n",
    "test_accuracy = compute_accuracy(test_labels, y_pred_test)\n",
    "\n",
    "print(\"Loss = {}, Train Accuracy = {} Test Accuracy = {}\".format(loss, train_accuracy, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf2",
   "language": "python",
   "display_name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}