{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as tr\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.core.display import display\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset downloading\n",
    "\n",
    "\n",
    "[SoF dataset](https://www.sites.google.com/view/sof-dataset)\n",
    "[YouTube Faces DB](https://www.cs.tau.ac.il/~wolf/ytfaces/)\n",
    "[CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize images\n",
    "If there is no necessity in the selection of the face, we can just resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(path, new_path, new_shape):\n",
    "    folder_names = os.listdir(path)\n",
    "    os.mkdir(new_path)\n",
    "\n",
    "    for name in folder_names:\n",
    "        os.mkdir(new_path + '/' + name)\n",
    "\n",
    "    for folder in folder_names:\n",
    "        images_names = os.listdir(path + '/' + folder)\n",
    "        for image_name in images_names:\n",
    "            image = io.imread(path + '/' + folder + '/' + image_name)\n",
    "            resized_image = resize(image, new_shape)\n",
    "            io.imsave(new_path + '/' + folder + '/' + image_name, resized_image, check_contrast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'saint george images'\n",
    "new_path = 'saint georges dataset'\n",
    "shape = (128, 128, 3)\n",
    "#resize_images(path,new_path,shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second step - face highlighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Main approach]( https://habr.com/ru/post/317798/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read computed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_template_path = 'preprocess weights/face_template.npy'\n",
    "dlib_predictor_path = \"preprocess weights/shape_predictor_68_face_landmarks.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find main points of the face "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible as dlib detector is pre-trained to find face points. It works on segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_me = \"display images/me.jpg\"\n",
    "image = io.imread(path_me)\n",
    "\n",
    "face_template = np.load(face_template_path)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(dlib_predictor_path)\n",
    "\n",
    "face_rects = list(detector(image, 1))\n",
    "face_rect = face_rects[0]\n",
    "\n",
    "points = predictor(image, face_rect)\n",
    "landmarks = np.array(list(map(lambda p: [p.x, p.y], points.parts())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look where points are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = Image.open(path_me)\n",
    "pix = photo.load()\n",
    "draw = ImageDraw.Draw(photo)\n",
    "N = 1\n",
    "\n",
    "for point in landmarks:\n",
    "    draw.ellipse(((point[0] - N, point[1] - N), (point[0] + N, point[1] + N)), fill=(0, 139, 139))\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "#plt.imshow(photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose 3 main points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose 3 main point on the image. It's not difficult as we know indeces of those points. Then to use Affine transformatiom need to choose 3 points, where the image would be wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n",
    "proper_landmarks = 227 * face_template[INNER_EYES_AND_BOTTOM_LIP]\n",
    "current_landmarks = landmarks[INNER_EYES_AND_BOTTOM_LIP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw 3 main point on the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = Image.open(path_me)\n",
    "pix = photo.load()\n",
    "draw = ImageDraw.Draw(photo)\n",
    "for point in current_landmarks:\n",
    "    draw.ellipse(((point[0] - N, point[1] - N), (point[0] + N, point[1] + N)), fill=(255, 0, 0))\n",
    "plt.figure(figsize=(7, 7))\n",
    "#plt.imshow(photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Affine transformation equetion consists of 2 matrices of points and T - transformation. $X^1, Y^1$ are points on original image and $X^0, Y^0$ are points on new, wrapped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_matrix = \"display images/matrix.jpg\"\n",
    "image_matrix = Image.open(path_matrix)\n",
    "display(image_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "A = np.hstack([current_landmarks, np.ones((3, 1))]).astype(np.float64)\n",
    "B = np.hstack([proper_landmarks, np.ones((3, 1))]).astype(np.float64)\n",
    "T = np.linalg.solve(A, B).T\n",
    "\n",
    "wrapped = tr.warp(\n",
    "    image,\n",
    "    tr.AffineTransform(T).inverse,\n",
    "    output_shape=(size, size),\n",
    "    mode='constant',\n",
    "    cval=0,\n",
    "    preserve_range=True\n",
    ")\n",
    "im = Image.fromarray(wrapped.astype(np.uint8))\n",
    "#display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring together all methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face_template_path = 'preprocess weights/face_template.npy'\n",
    "dlib_predictor_path = \"preprocess weights/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "\n",
    "class Transformer():\n",
    "\n",
    "    def __init__(self, face_template_path, dlib_predictor_path):\n",
    "        self.face_template = np.load(face_template_path)\n",
    "        self.predictor = dlib.shape_predictor(dlib_predictor_path)\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n",
    "\n",
    "    def crop_face(self, image, size=(128, 128)):\n",
    "\n",
    "        face_rects = list(self.detector(image, 1))\n",
    "        if len(face_rects) == 1:\n",
    "            face_rect = face_rects[0]\n",
    "\n",
    "            points = self.predictor(image, face_rect)\n",
    "            landmarks = np.array(list(map(lambda p: [p.x, p.y], points.parts())))\n",
    "\n",
    "            proper_landmarks = size * self.face_template[self.INNER_EYES_AND_BOTTOM_LIP]\n",
    "            current_landmarks = landmarks[self.INNER_EYES_AND_BOTTOM_LIP]\n",
    "\n",
    "            A = np.hstack([current_landmarks, np.ones((3, 1))]).astype(np.float64)\n",
    "            B = np.hstack([proper_landmarks, np.ones((3, 1))]).astype(np.float64)\n",
    "            T = np.linalg.solve(A, B).T\n",
    "\n",
    "            wrapped = tr.warp(\n",
    "                image,\n",
    "                tr.AffineTransform(T).inverse,\n",
    "                output_shape=size,\n",
    "                mode='constant',\n",
    "                cval=0,\n",
    "                preserve_range=True\n",
    "            )\n",
    "\n",
    "            return wrapped.astype(np.uint8)\n",
    "        else:\n",
    "            return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformer = Transformer(face_template_path, dlib_predictor_path)\n",
    "img = io.imread('display images/me.jpg')\n",
    "res = transformer.crop_face(img, size=(256, 256))\n",
    "plt.imshow(res)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sof faces crop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = 'datasets/sof'\n",
    "new_path = 'sof dataset test'\n",
    "files_names = os.listdir(path)\n",
    "\n",
    "transformer = Transformer(face_template_path, dlib_predictor_path)\n",
    "\n",
    "folder_names = list(set([name[:4] for name in files_names]))\n",
    "\n",
    "if not os.path.exists(new_path):\n",
    "    os.mkdir(new_path)\n",
    "\n",
    "for i, name in enumerate(folder_names):\n",
    "    class_path = new_path + '/' + str(i)\n",
    "    if not os.path.exists(class_path):\n",
    "        os.mkdir(class_path)\n",
    "\n",
    "for name in files_names:\n",
    "    image = io.imread(path + '/' + name)\n",
    "    way = name[:4]\n",
    "    cropped = transformer.crop_face(image)\n",
    "    if cropped is not None:\n",
    "        namel = name[6:10]\n",
    "        io.imsave(new_path + '/' + str(folder_names.index(way)) + '/' + str(namel) + '.jpg',cropped)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove classes, where the number of images is smaller, then threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "path = 'sof dataset'\n",
    "N = 11  # minimal number of images of the class\n",
    "files_names = os.listdir(path)\n",
    "folder_names = list(set([name[:4] for name in files_names]))\n",
    "for i in folder_names:\n",
    "    if len(os.listdir(path + '/' + str(i))) < N:\n",
    "        for j in os.listdir(path + '/' + str(i)):\n",
    "            os.remove(path + '/' + str(i) + '/' + j)\n",
    "        os.rmdir(path + '/' + str(i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}